{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM with Focal Loss for Multiclass classification problems\n",
    "\n",
    "Let me show how to adapt the Focal Loss implementation for binary classification to a multiclass classification problem.\n",
    "\n",
    "The idea is to face the problem using the Binary Cross Entropy With Logits (borrowing from `Pytorch` notation `BCEWithLogitsLoss`). \n",
    "\n",
    "$$\n",
    "loss = -[y_{\\text true} \\cdot log\\sigma(x) + (1-y_{\\text true}) \\cdot log(1-\\sigma(x))] \n",
    "$$\n",
    "\n",
    "Where $\\sigma$ is the sigmoid function\n",
    "\n",
    "For example, let's assume we have a problem with 10 classes and we have two samples/observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.random.choice(11, (1,2))\n",
    "# from -2 to 2 to illustrate the fact the preds coming from lightGBM when using custom losses are NOT probs\n",
    "y_pred = np.random.uniform(low=-2, high=2, size=(2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.29824142, -1.3932109 , -0.3560161 , -1.83911858,  1.25744599,\n",
       "         1.6930721 , -0.41016591, -0.76641368,  0.91205306,  1.38041321],\n",
       "       [ 1.49556617, -1.1040521 ,  0.5010648 , -1.1783269 , -0.63015764,\n",
       "        -0.56899891, -0.76107954, -1.66164642, -0.23192115, -1.93675266]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Â predictions\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1./(1. +  np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels one-hot encoded\n",
    "y_true_oh = np.eye(10)[y_true][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787502349207979"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BCEWithLogitsLoss\n",
    "( -( y_true_oh * np.log(sigmoid(y_pred)) + (1-y_true_oh) * np.log(1-sigmoid(y_pred)) ) ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Focal Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_lgb(y_pred, dtrain, alpha, gamma, num_class):\n",
    "    \"\"\"\n",
    "    Focal Loss for lightgbm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred: numpy.ndarray\n",
    "        array with the predictions\n",
    "    dtrain: lightgbm.Dataset\n",
    "    alpha, gamma: float\n",
    "        See original paper https://arxiv.org/pdf/1708.02002.pdf\n",
    "    num_class: int\n",
    "        number of classes\n",
    "    \"\"\"\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    # N observations x num_class arrays\n",
    "    y_true = np.eye(num_class)[y_true.astype('int')]\n",
    "    y_pred = y_pred.reshape(-1,num_class)\n",
    "    # alpha and gamma multiplicative factors with BCEWithLogitsLoss\n",
    "    def fl(x,t):\n",
    "        p = 1/(1+np.exp(-x))\n",
    "        return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) )\n",
    "    partial_fl = lambda x: fl(x, y_true)\n",
    "    grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
    "    hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
    "    # flatten in column-major (Fortran-style) order\n",
    "    return grad.flatten('F'), hess.flatten('F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it really. Now one would want/need the corresponding evalulation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_lgb_eval_error(y_pred, dtrain, alpha, gamma, num_class):\n",
    "    \"\"\"\n",
    "    Focal Loss for lightgbm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred: numpy.ndarray\n",
    "        array with the predictions\n",
    "    dtrain: lightgbm.Dataset\n",
    "    alpha, gamma: float\n",
    "        See original paper https://arxiv.org/pdf/1708.02002.pdf\n",
    "    num_class: int\n",
    "        number of classes\n",
    "    \"\"\"\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    y_true = np.eye(num_class)[y_true.astype('int')]\n",
    "    y_pred = y_pred.reshape(-1,num_class)\n",
    "    p = 1/(1+np.exp(-y_pred))\n",
    "    loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) )\n",
    "    # a variant can be np.sum(loss)/num_class\n",
    "    return 'focal_loss', np.mean(loss), False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from scipy.misc import derivative\n",
    "\n",
    "# very inadequate dataset as is perfectly balanced, but just to illustrate\n",
    "iris = datasets.load_iris()\n",
    "X_org = iris.data\n",
    "y_org = iris.target\n",
    "\n",
    "# shuffle...makes me feel good\n",
    "x = np.hstack([X_org,y_org.reshape(-1, 1)])\n",
    "np.random.shuffle(x)\n",
    "\n",
    "X = x[:, :4]\n",
    "y = x[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "lgbtrain = lgb.Dataset(X_tr, y_tr, free_raw_data=True)\n",
    "lgbeval = lgb.Dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's focal_loss: 0.101055\n",
      "[2]\tvalid_0's focal_loss: 0.101026\n",
      "[3]\tvalid_0's focal_loss: 0.100997\n",
      "[4]\tvalid_0's focal_loss: 0.100968\n",
      "[5]\tvalid_0's focal_loss: 0.100939\n"
     ]
    }
   ],
   "source": [
    "focal_loss = lambda x,y: focal_loss_lgb(x, y, 0.25, 2., 3)\n",
    "eval_error = lambda x,y: focal_loss_lgb_eval_error(x, y, 0.25, 2., 3)\n",
    "params  = {'learning_rate':0.001, 'num_boost_round':5, 'num_class':3}\n",
    "# model = lgb.train(params, lgbtrain, fobj=focal_loss)\n",
    "model = lgb.train(params, lgbtrain, valid_sets=[lgbeval], fobj=focal_loss, feval=eval_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, np.argmax(sigmoid(model.predict(X_val)), axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
